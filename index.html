<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <!-- <link rel="shortcut icon" type="image/x-icon" href="images/cv.ico"> -->
    <title>Jae-Sung Bae</title>

    <!-- style -->
    <link rel="stylesheet" type="text/css" href="style.css">

    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
        integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous"> -->
    <!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous"> -->
</head>

<body>
    <div class="continaer page-wrapper">
        <!-- <div class="row"> -->
        <div class="container" id="top">
            <h1>Jaesung Bae</h1>
            <p>jb82 [at] illinois [dot] edu</p>
            <!--- <p class="lead">undergrad studying cs and math<br> </p> -->
        </div>
        <div class="container page-wrapper">
            <ul>
                <!-- <li><a href="index.html">Home</a></li>  -->
                <!-- <li> <a href="data/cv_jaesungbae_240208.pdf">CV</a></li>  -->
                <li> <a href="cv.html">CV</a></li>
                <li> <a href="https://scholar.google.co.kr/citations?user=ay1zanAAAAAJ&hl=en">Google Scholar</a></li>
                <li> <a href="http://www.linkedin.com/in/jaesung-bae-955410157">LinkedIn</a> </li>
                <li> <a href="#projects">Projects</a></li>
                <li> <a href="#publications">Publications</a></li>
                <li> <a href="#invited_talks">Invited Talks</a></li>
                <!-- <li> <a href="https://github.com/czlwang">Github</a></li> 
        <li><a href="art/index.html">Drawing</a></li> 
        <li><a href="zettel/index.html">Zettelkasten</a></li> 
        <li><a href="misc/index.html">Misc</a></li>  -->
            </ul>
        </div>

        <div class="container">
            <h2>About</h2>
            <hr>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        I am a PhD student in the <a href="https://siebelschool.illinois.edu/">Computer Science (CS)</a>
                        Department at the <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>,
                        advised by <a href="https://minjekim.com/">Prof. Minje Kim</a> and <a
                            href="https://paris.cs.illinois.edu/">Prof. Paris Smaragdis</a>.
                        <br>
                        &nbsp&nbsp Previously I worked as a speech AI researcher at <a
                            href="https://research.samsung.com/">Samsung Research</a> .
                        My main research topics incldue personalized and zero-shot on-device TTS systems, and I’m proud
                        to have contributed to the TTS systems integrated in the Galaxy S24.
                        Before that, I worked at <a href="https://kr.ncsoft.com/en/index.do">NCSOFT</a>, a game company,
                        where I primarily
                        studied expressive TTS and prosody controllable TTS systems.
                        I earned my MS in <a href="https://ee.kaist.ac.kr/en/">Electrical Engineering</a> from <a
                            href="https://www.kaist.ac.kr/en/">KAIST</a>,
                        where I was advised by Prof. Daeshik Kim in the <a
                            href="http://brain.kaist.ac.kr/brain/main.php">BREIL lab</a>, and my
                        BS in <a href="https://ee.yonsei.ac.kr/ee_en/index.do">Electrical and Electronic Engineering</a>
                        from <a href="https://www.yonsei.ac.kr/en_sc/">Yonsei University</a>.
                        <br>
                        &nbsp&nbsp I am interested in speech synthesis and speech representation learing of prosody and
                        spaeker identity.
                        Currently, I am expanding my interests to generative models for data augmentation, multi-modal
                        AI, and other areas in speech processing.
                        <!-- combining speech synthesis with techniques from various fields, 
                    such as spontaneous speech-to-speech, multimodal generation, video dubbing, etc. -->
                    </p>
                    <p>
                        &nbsp&nbsp Below shows my <a href="#projects">projects</a>, <a
                            href="#publications">publications</a>, <a href="#invited_talks">invited talks</a>, and <a
                            href="#academic_services">academic services</a>. Please refere to my <a
                            href="cv.html"><strong>CV</strong></a> for further details.
                    </p>
                </div>
                <div class="col-md-3">
                    <img src="images/profile.jpg" class="img-circle" width="275px" height="275px">
                </div>
            </div>
        </div>

        <div class="container" id="projects">
            <h2>News</h2>
            <hr>
            <div class="row">
                <ol>
                    <li>(Dec 2024) I am organizing the <i>"ICASSP 2025 Generative Data Augmentation Challenge:
                            Zero-Shot Speech Synthesis for Personalized Speech Enhancement."</i> Looking forward to
                        your participation! <a href="https://sites.google.com/view/genda2025/pse">[link]</a>
                    </li>
                    <li>(Aug 2024) Starting my PhD at the <i>University of Illinois Urbana-Champaign (UIUC).</i>
                    </li>
                    <li>(Dec 2023) Two papers have been accepted to <i>ICASSP 2024!</i> (one first author, one
                        second author)</li>
                </ol>
            </div>
        </div>

        <div class="container" id="publications">
            <h2>Publications</h2>
            <hr>
            <div class="row">
                <div class="col-md-9">
                    <h5>
                        *: Equal Contribution
                    </h5>
                </div>
                <div class="col-md-3 col-md-offset-3">
                    <p><a class="btn btn-default" href="#top" role="button">Go to top</a></p>
                </div>
            </div>
            <!-- <a href=""> </a><br> -->
            <!-- </p> -->
            <div class="row">
                <div class="col-md-10 pub-year">
                    2025 </div>
            </div>
            <div class="row">
                <div class="col-md-10">
                    <h4>
                        Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for Personalized Speech
                        Enhancement
                        <span class="h5"><br>
                            <b>Jae-Sung Bae</b>, Anastasia Kuznetsova, Dinesh Manocha, John Hershey, Trausti
                            Kristjansson, and Minje Kim <br>
                            <!-- <i>arXiv preprint arXiv: 2310.03538,</i> 2023. <span style="border-radius: 5px; background: #FF0000; color:#FFFFFF; padding: 2px;">Accepted to ICASSP 2024</span> <br> -->
                            <i>In Proc. of the IEEE Int. Conf. on Acoustics, Speech, and Signal
                                Processing Workshops (ICASSPW): Generative Data Augmentation for Real-World Signal
                                Processing Applications (GenDA 2025),</i> 2025. <br>

                            <a href="https://arxiv.org/abs/2501.13372">[paper]</a>
                            <a href="https://github.com/JaesungBae/GenDA-Challenge-25-ZSTTS-PSE">[code]</a>
                            <a href="https://sites.google.com/view/genda2025/pse?authuser=0">[website]</a>
                        </span>
                    </h4>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-10 pub-year">
                    2024
                </div>
            </div>
            <div class="row">
                <div class="col-md-10">
                    <h4>
                        Latent Filling: Latent Space Data Augmentation for Zero-shot Speech Synthesis
                        <span class="h5"><br>
                            <b>Jae-Sung Bae</b>, Joun Yeop Lee, Ji-Hyun Lee, Seongkyu Mun, Taehwa Kang, Hoon-Young
                            Cho,
                            Chanwoo Kim<br>
                            <!-- <i>arXiv preprint arXiv: 2310.03538,</i> 2023. <span style="border-radius: 5px; background: #FF0000; color:#FFFFFF; padding: 2px;">Accepted to ICASSP 2024</span> <br> -->
                            <i>In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),</i>
                            2024.<br>

                            <a href="https://arxiv.org/abs/2310.03538">[paper]</a>
                            <a href="https://srtts.github.io/latent-filling/index.html">[demo]</a>
                        </span>
                    </h4>
                </div>
            </div>
            <div class="row">
                <div class="col-md-10">
                    <h4>
                        MELS-TTS : Multi-Emotion Multi-Lingual Multi-Speaker Text-to-Speech System via Disentangled
                        Style Tokens
                        <span class="h5"><br>
                            Heejin Choi, <b>Jae-Sung Bae</b>, Joun Yeop Lee, Seongkyu Mun, Jihwan Lee, Hoon-Young
                            Cho,
                            Chanwoo Kim<br>
                            <!-- <span style="border-radius: 5px; background: #FF0000; color:#FFFFFF; padding: 2px;">Accepted to ICASSP 2024</span> <br> -->
                            <i>In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),</i>
                            2024.<br>
                        </span>
                    </h4>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-10 pub-year">
                    2023
                </div>
            </div>
            <div class="row">
                <div class="col-md-10">
                    <h4>
                        Hierarchical Timbre-Cadence Speaker Encoder for Zero-shot Speech Synthesis
                        <span class="h5"><br>
                            Joun Yeop Lee, <b>Jae-Sung Bae</b>, Seongkyu Mun, Jihwan Lee, Ji-Hyun Lee, Hoon-Young
                            Cho,
                            Chanwoo Kim<br>
                            In<i> Proc. INTERSPEECH</i>, 2023. <br>
                            <a
                                href="https://www.isca-speech.org/archive/interspeech_2023/lee23f_interspeech.html">[paper]</a>
                            <a href="https://srtts.github.io/tc-zstts/">[demo]</a>
                        </span>
                    </h4>
                </div>
            </div>
            <div class="row">
                <div class="col-md-10">
                    <h4>
                        Avocodo: Generative Adversarial Network for Artifact-free Vocoder
                        <span class="h5"><br>
                            Taejun Bak, Junmo Lee, Hanbin Bae, Jinhyeok Yang, <b>Jae-Sung Bae</b>, Young-Sun Joo<br>
                            In<i> Proc. AAAI</i>, 2023.<br>
                            </small>
                            <a href="https://arxiv.org/abs/2206.13404">[paper]</a>
                            <a href="https://nc-ai.github.io/speech/publications/Avocodo/index.html">[demo]</a>
                            <a href="https://github.com/ncsoft/avocodo">[code]</a>
                    </h4>
                </div>
            </div>
            <br>
            <!-- <hr> -->
            <div class="row">
                <div class="col-md-10 pub-year">
                    2022
                </div>
            </div>
            <div class="row">
                <div class="col-md-12">
                    <h4>
                        Hierarchical and Multi-Scale Variational Autoencoder for Diverse and Natural
                        Non-Autoregressive
                        Text-to-Speech
                        <span class="h5"><br>
                            <b>Jae-Sung Bae</b>, Jinhyeok Yang, Tae-Jun Bak, Young-Sun Joo<br>
                            In <i>Proc. INTERSPEECH</i>, 2022.<br>
                            <a href="https://arxiv.org/abs/2204.04004">[paper]</a>
                            <a href="https://nc-ai.github.io/speech/publications/himuv-tts/">[demo]</a>
                            <a href="https://youtu.be/3U5cEu0gFYY ">[video]</a>
                        </span>
                    </h4>
                </div>
            </div>
            <div class="row">
                <div class="col-md-10">
                    <h4>
                        Into-TTS : Intonation Template Based Prosody Control System
                        <span class="h5"><br>
                            Jihwan Lee, Joun Yeop Lee, Heejin Choi, Seongkyu Mun, Sangjun Park, <b>Jae-Sung Bae</b>,
                            Chanwoo Kim<br>
                            <i>arXiv preprint arXiv:2204.01271,</i> 2022.<br>
                            <a href="https://arxiv.org/abs/2204.01271">[paper]</a>
                            <a href="https://srtts.github.io/IntoTTS/">[demo]</a>
                        </span>
                    </h4>
                </div>
            </div>
            <br>
            <!-- <hr> -->
            <div class="row">
                <div class="col-md-10 pub-year">
                    2021
                </div>
            </div>
            <div class="row">
                <div class="col-md-10">
                    <h4>
                        Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech
                        <span class="h5"><br>
                            <b>Jae-Sung Bae</b>, Tae-Jun Bak, Young-Sun Joo, and Hoon-Young Cho
                            <br>In <i>Proc. INTERSPEECH</i>, 2021.<br>
                            <a href="https://arxiv.org/abs/2106.15144">[paper]</a>
                            <a
                                href="https://nc-ai.github.io/speech/publications/hierarchical-transformers-tts/">[demo]</a>
                    </h4>
                </div>
            </div>

            <div class="row">
                <div class="col-md-10">
                    <h4>
                        GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis
                        <span class="h5"><br>
                            Jinhyeok Yang*, <b>Jae-Sung Bae*</b>, Taejun Bak, Youngik Kim, and Hoon-Young Cho<br>
                            In <i>Proc. INTERSPEECH</i>, 2021.<br>
                            <a href="https://arxiv.org/abs/2106.15153">[paper]</a>
                            <a href="https://nc-ai.github.io/speech/publications/ganspeech/">[demo]</a>
                    </h4>
                    <h5>
                    </h5>
                </div>
            </div>

            <div class="row">
                <div class="col-md-10">
                    <h4>
                        FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis
                        <span class="h5"><br>
                            Taejun Bak, <b>Jae-Sung Bae</b>, Hanbin Bae, Young-Ik Kim, and Hoon-Young Cho<br>
                            In <i>Proc. INTERSPEECH</i>, 2021.<br>
                            <a href="https://arxiv.org/abs/2106.15123">[paper]</a>
                            <a href="https://nc-ai.github.io/speech/publications/fastpitchformant/">[demo]</a>
                    </h4>
                </div>
            </div>

            <div class="row">
                <div class="col-md-10">
                    <h4>
                        A Neural Text-to-Speech Model Utilizing Broadcast Data Mixed with Background Music
                        <span class="h5"><br>
                            Hanbin Bae, <b>Jae-Sung Bae</b>, Young-Sun Joo, Young-Ik Kim, and Hoon-Young Cho<br>
                            In <i>Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</i>,
                            2021.<br>
                            <a href="https://arxiv.org/abs/2103.03049">[paper]</a>
                            <a href="https://nc-ai.github.io/speech/publications/tts-with-bgm-data/">[demo]</a>
                    </h4>
                    <h5>
                    </h5>
                </div>
            </div>

            <br>
            <!-- <hr> -->

            <div class="row">
                <div class="col-md-10 pub-year">
                    2020
                </div>
            </div>

            <div class="row">
                <div class="col-md-10">
                    <h4>
                        Speaking Speed Control of End-to-End Speech Synthesis using Sentence-Level Conditioning
                        <span class="h5"><br>
                            <b>Jae-Sung Bae</b>, Hanbin Bae, Young-Sun Joo, Junmo Lee, Gyeong-Hoon Lee, Hoon-Young
                            Cho<br>
                            In <i>Proc. INTERSPEECH</i>, 2020.<br>
                            <a href="https://arxiv.org/abs/2007.15281">[paper]</a>
                            <a href="https://nc-ai.github.io/speech/publications/speed-controllable-tts/">[demo]</a>
                            <a href="https://youtu.be/WyDfc53Ez_A ">[video]</a>
                    </h4>
                </div>
            </div>

            <br>
            <!-- <hr> -->
            <!-- <hr> -->

            <div class="row">
                <div class="col-md-10 pub-year">
                    2018
                </div>
            </div>

            <div class="row">
                <div class="col-md-10">
                    <h4>
                        End-to-End Speech Command Recognition with Capsule Network
                        <span class="h5"><br>
                            <b>Jae-Sung Bae</b>, Dae-Shik Kim<br>
                            In <i>Proc. INTERSPEECH</i>, 2018.<br>
                            <a
                                href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/bae18_interspeech.pdf">[paper]</a>
                    </h4>
                </div>
            </div>
        </div>

        <div class="container" id="projects">
            <h2>Projects</h2>
            <hr>
            <div class="row">
                <div class="col-md-9">
                    <p>
                        You can click each project and check demos and further information.
                    </p>
                </div>
                <div class="col-md-3 col-md-offset-3">
                    <p><a class="btn btn-default" href="#top" role="button">Go to top</a></p>
                </div>
            </div>

            <div class="row">
                <div class="col-md-3" OnClick="location.href='projects/5_live_translation.html'" style="cursor:pointer"
                    id="project:5_live_translation">
                    <img class="main_project_img" src="images/galaxy_s24_live_translation.jpg" width="240px"
                        height="160px">
                </div>
                <div class="col-md-8" OnClick="location.href='projects/5_live_translation.html'" style="cursor:pointer"
                    id="project:5_live_translation">
                    <h3 style="margin-top:0px;">On-device TTS System in various languages for Galaxy S24's Live
                        Translation<br><small>Mar 2023 - Jan 2024 (@Samsung Research)</small></h3>
                    <p>
                        I contributed to the research and development of an on-device TTS system in eight different
                        languages,
                        which is included as a Live Translation feature and introduced as a main AI feature in the
                        Galaxy S24.
                        My contribution involved enhancing the model architecture and achieving a high-quality TTS
                        system that supports various languages
                        with a reduced model size.
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-3" OnClick="location.href='projects/4_ptts.html'" style="cursor:pointer"
                    id="project:4_ptts">
                    <img class="main_project_img" src="images/bixby_custom_voice.png" width="240px" height="160px">
                </div>
                <div class="col-md-8" OnClick="location.href='projects/4_ptts.html'" style="cursor:pointer"
                    id="project:4_ptts">
                    <h3 style="margin-top:0px;">On-device Personalized TTS System for Bixby Custom Voice
                        Creation<br><small>May 2022 - Jan 2024 (@Samsung Research)</small></h3>
                    <p>
                        I contributed to the research and development of an on-device personalized TTS system, which was
                        integrated into Samsung
                        Galaxy Bixby's Custom Voice Creation and utilized within Bixby Text-call functionality. This
                        system can create a
                        personalized TTS system by fine-tuning the TTS directly on the user’s device with just 10
                        utterances.
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-3" OnClick="location.href='projects/controllable.html'" style="cursor:pointer"
                    id="project:universe">
                    <img class="main_project_img" src="images/thumbnail_controllable.jpg" width="240px" height="100px">
                </div>
                <div class="col-md-8" OnClick="location.href='projects/controllable.html'" style="cursor:pointer"
                    id="project:controllable">
                    <h3 style="margin-top:0px;">Fine-grained Prosody Control of TTS System (prototype web
                        service)<br><small>Mar 2021 - Apr 2022 (@NCSOFT)</small></h3>
                    <p>
                        I conducted research and developed a TTS system that is capable of controlling the prosody of
                        speech in a fine-grained level.
                        With this system, users were able to modify the speech to have desired prosody. This system is
                        released as an in-company web service and was widely used to make an guide videos of NCSOFT's
                        game.
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-3" OnClick="location.href='projects/uninverse.html'" style="cursor:pointer"
                    id="project:universe">
                    <img class="main_project_img" src="images/universe_1.jpg" width="240px" height="160px">
                </div>
                <div class="col-md-8" OnClick="location.href='projects/uninverse.html'" style="cursor:pointer"
                    id="project:universe">
                    <h3 style="margin-top:0px;">TTS System for K-pop Fandom Platform, “UNIVERSE” (live
                        service)<br><small>Mar 2019 - Apr 2022 (@NCSOFT)</small></h3>
                    <p>
                        I contributed to the research and development of a multi-speaker TTS system replicating the
                        voices of numerous K-pop artists, approximately 100 in total, within a single TTS system.
                        This TTS system was used in "UNIVERSE" service, which is a K-pop fan community platform.
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-3" OnClick="location.href='projects/baseball.html'" style="cursor:pointer"
                    id="project:baseball">
                    <video class="main_project_img" width="240" height="160" autoplay loop>
                        <source src="images/baseball_example_png.mp4" type="video/mp4" width="240px" height="160px">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="col-md-8" OnClick="location.href='projects/baseball.html'" style="cursor:pointer"
                    id="project:baseball">
                    <h3 style="margin-top:0px;">TTS System in Baseball Broadcast Scenario<br><small>Mar 2019 - Mar 2021
                            (@NCSOFT)</small></h3>
                    <p>
                        I researched and developed an expressive TTS system that can generate speech with dynamic
                        expressions suitable for diverse baseball situations. I published several demos on NCSOFT’s
                        official blog and news articles.
                        <b>Kindly recommand to click this project, and see the demo videos.</b>
                    </p>
                </div>
            </div>
        </div>

        <div class="container" id="invited_talks">
            <h2>Invited Talks</h2>
            <hr>
            <div class="row">
                <div class="col-md-9">
                    <h4>
                        <a href="https://youtu.be/iFtZqjedoWE">End-to-End Speech Command Recognition with Capsule
                            Network</a>
                        <br>
                        <span class="h5"><b>NAVER Corp.,</b> Seong-Nam, Republic of Korea</span>
                        <br>
                        <small>Sep 2018</small>
                    </h4>
                </div>
                <div class="col-md-1 col-md-offset-1">
                    <p><a class="btn btn-default" href="#top" role="button">Go to top</a></p>
                </div>
            </div>
        </div>

        <div class="container" id="academic_services">
            <h2>Academic Services</h2>
            <hr>
            <div class="row">
                <ol>
                    <li>Challenge Organizer on ICASSP 2025 Generative Data Augmentation for Real-World Signal
                        Processing Applications (GenDA 2025) Workshop: Zero-Shot
                        Speech Synthesis for Personalized Speech Enhancement <a
                            href="https://sites.google.com/view/genda2025/pse">[link]</a>
                    </li>
                    <li>Reviewer: AAAI 2025</li>

                </ol>

            </div>
        </div>



        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
            crossorigin="anonymous"></script>
    </div> <!--   page-wrapper  -->
</body>

</html>
